<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="RL-Based Trading Agent — DQN agent with custom reward shaping for simulated equity trading.">
  <title>RL-Based Trading Agent — Javian</title>
  <link rel="stylesheet" href="../style.css">
</head>
<body>

  <a href="#main" class="skip-link">Skip to content</a>

  <nav class="nav" role="navigation" aria-label="Main navigation">
    <div class="nav__inner">
      <a href="../index.html" class="nav__name">Javian</a>
      <ul class="nav__links">
        <li><a href="../index.html">Home</a></li>
        <li><a href="../projects.html" class="active">Projects</a></li>
        <li><a href="../about.html">About</a></li>
        <li><a href="../contact.html">Contact</a></li>
      </ul>
      <button class="nav__toggle" aria-label="Toggle menu" aria-expanded="false">
        <span></span><span></span><span></span>
      </button>
    </div>
    <div class="nav__mobile-menu" id="mobile-menu">
      <a href="../index.html">Home</a>
      <a href="../projects.html" class="active">Projects</a>
      <a href="../about.html">About</a>
      <a href="../contact.html">Contact</a>
    </div>
  </nav>

  <main id="main">

    <section class="project-hero">
      <div class="container">
        <a href="../projects.html" class="project-hero__back">← Back to projects</a>
        <h1 class="project-hero__title">RL-Based Trading Agent</h1>
        <dl class="project-hero__meta">
          <div><dt>Role:</dt><dd>Solo developer</dd></div>
          <div><dt>Type:</dt><dd>Independent exploration project</dd></div>
          <div><dt>Stack:</dt><dd>Python, PyTorch, OpenAI Gym, Pandas</dd></div>
        </dl>
        <div class="project-card__tags">
          <span class="tag">Reinforcement Learning</span>
          <span class="tag">DQN</span>
          <span class="tag">PyTorch</span>
          <span class="tag">Financial Markets</span>
        </div>
      </div>
    </section>

    <section class="project-section">
      <div class="container">
        <span class="project-section__label">Problem Statement</span>
        <h2 class="project-section__title">Can an RL agent learn a trading policy from price data alone?</h2>
        <p>
          Supervised learning predicts future prices, then a separate layer decides what to do with 
          the prediction. Reinforcement learning merges both steps: the agent directly learns which 
          actions (buy, hold, sell) maximize a reward signal over time. The question was whether a 
          DQN agent trained on historical equity data could learn a policy that outperforms a 
          simple buy-and-hold baseline.
        </p>
        <p>
          This is an exploration project — the goal was to understand how RL applies to financial 
          environments, not to build a production trading system.
        </p>
      </div>
    </section>

    <section class="project-section">
      <div class="container">
        <span class="project-section__label">Why This Is Hard</span>
        <h2 class="project-section__title">Sparse rewards, non-stationarity, and reward hacking</h2>
        <p>
          In trading, the reward signal (profit or loss) is delayed and sparse. The agent might make 
          50 decisions before the consequence of a single buy becomes clear. This makes credit 
          assignment — figuring out which past action caused the current outcome — extremely 
          difficult for standard RL algorithms.
        </p>
        <p>
          The environment is non-stationary. Market dynamics shift over time, so a policy learned 
          on 2020 data may not generalize to 2023. Unlike a game with fixed rules, the "rules" of 
          the financial market change continuously.
        </p>
        <p>
          Naive reward functions (maximize total return) lead to reward hacking: the agent learns 
          to take extreme positions that work in training data but blow up out-of-sample. Shaping 
          the reward to penalize drawdowns and excessive trading frequency is critical.
        </p>
      </div>
    </section>

    <section class="project-section">
      <div class="container">
        <span class="project-section__label">Approach & Key Decisions</span>
        <h2 class="project-section__title">DQN with risk-adjusted reward shaping</h2>
        <p>
          I built a custom Gym environment that simulates equity trading for a single stock. The 
          state space includes the last 30 days of returns, rolling volatility, current position 
          (long, flat, or short), and unrealized P&L. The action space is discrete: buy, hold, sell.
        </p>
        <p>
          The key design decision was the reward function. Instead of raw P&L, I used a 
          risk-adjusted reward: daily return minus a penalty proportional to drawdown from peak 
          equity, plus a small penalty for each trade (to discourage churning). This 
          encourages the agent to seek returns while actively managing downside risk.
        </p>
        <p>
          I used a standard DQN architecture with experience replay and target network updates. 
          The network has 3 hidden layers (128, 64, 32 units) with ReLU activations. I trained for 
          500 episodes with epsilon-greedy exploration decaying from 1.0 to 0.05.
        </p>

        <div class="diagram">
          <svg viewBox="0 0 800 220" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="RL training loop showing environment, agent, and reward shaping">
            <!-- Environment -->
            <rect x="40" y="30" width="160" height="70" rx="3" fill="#f2f2f2" stroke="#111" stroke-width="1.5"/>
            <text x="120" y="58" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="12" font-weight="600" fill="#111">Trading Environment</text>
            <text x="120" y="78" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="9" fill="#888">Custom OpenAI Gym</text>

            <!-- State arrow down -->
            <line x1="120" y1="100" x2="120" y2="130" stroke="#ccc" stroke-width="1.5" marker-end="url(#arrow5)"/>
            <text x="145" y="118" font-family="DM Sans, sans-serif" font-size="9" fill="#888">state</text>

            <!-- Agent -->
            <rect x="40" y="130" width="160" height="70" rx="3" fill="#111" stroke="#111" stroke-width="1.5"/>
            <text x="120" y="158" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="12" font-weight="600" fill="#fafafa">DQN Agent</text>
            <text x="120" y="178" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="9" fill="#aaa">128→64→32 + replay</text>

            <!-- Action arrow up-right -->
            <path d="M200,165 L280,165 L280,65 L200,65" fill="none" stroke="#ccc" stroke-width="1.5" marker-end="url(#arrow5)"/>
            <text x="290" y="120" font-family="DM Sans, sans-serif" font-size="9" fill="#888">action</text>

            <!-- Reward shaping box -->
            <rect x="360" y="50" width="180" height="130" rx="3" fill="none" stroke="#111" stroke-width="1.5" stroke-dasharray="4,3"/>
            <text x="450" y="75" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="11" font-weight="600" fill="#111">Reward Shaping</text>
            <text x="450" y="100" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="10" fill="#555">r = daily_return</text>
            <text x="450" y="118" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="10" fill="#555">  − λ₁ · drawdown</text>
            <text x="450" y="136" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="10" fill="#555">  − λ₂ · trade_penalty</text>
            <text x="450" y="168" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="9" fill="#888">λ₁=0.5, λ₂=0.001</text>

            <!-- Reward arrow -->
            <line x1="360" y1="165" x2="200" y2="165" stroke="#ccc" stroke-width="1.5" marker-end="url(#arrow5)"/>
            <text x="310" y="158" font-family="DM Sans, sans-serif" font-size="9" fill="#888">reward</text>

            <!-- Evaluation -->
            <rect x="600" y="80" width="160" height="70" rx="3" fill="none" stroke="#111" stroke-width="1.5"/>
            <text x="680" y="108" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="11" font-weight="600" fill="#111">Evaluation</text>
            <text x="680" y="128" text-anchor="middle" font-family="DM Sans, sans-serif" font-size="9" fill="#888">vs Buy-and-Hold baseline</text>

            <line x1="540" y1="115" x2="600" y2="115" stroke="#ccc" stroke-width="1.5" marker-end="url(#arrow5)"/>

            <defs>
              <marker id="arrow5" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
                <path d="M0,0 L8,3 L0,6" fill="none" stroke="#ccc" stroke-width="1"/>
              </marker>
            </defs>
          </svg>
        </div>
      </div>
    </section>

    <section class="project-section">
      <div class="container">
        <span class="project-section__label">Results & Metrics</span>
        <h2 class="project-section__title">Outperformed baseline in trending markets, not in flat ones</h2>

        <div class="metrics">
          <div class="metric">
            <div class="metric__value">+7.2%</div>
            <div class="metric__label">Annualized excess return (trending)</div>
          </div>
          <div class="metric">
            <div class="metric__value">−18%</div>
            <div class="metric__label">Max drawdown reduction</div>
          </div>
          <div class="metric">
            <div class="metric__value">0.85</div>
            <div class="metric__label">Sharpe ratio (agent)</div>
          </div>
          <div class="metric">
            <div class="metric__value">500</div>
            <div class="metric__label">Training episodes</div>
          </div>
        </div>

        <p>
          In periods with clear trends (up or down), the agent outperformed buy-and-hold by 
          generating 7.2% annualized excess return with an 18% smaller maximum drawdown. The 
          Sharpe ratio of 0.85 indicates a modest risk-adjusted return improvement.
        </p>
        <p>
          In sideways or range-bound markets, the agent performed roughly equal to or slightly 
          worse than buy-and-hold, as expected — the reward shaping penalizes frequent trades, 
          and in a flat market, most trades are noise.
        </p>
      </div>
    </section>

    <section class="project-section">
      <div class="container">
        <span class="project-section__label">Trade-offs & Limitations</span>
        <h2 class="project-section__title">Simulation gap and single-asset constraint</h2>
        <p>
          The simulation does not model transaction costs, slippage, or market impact. In real 
          markets, every trade has a cost, and the agent's frequent position changes would erode 
          returns significantly. The trade penalty in the reward function is a crude proxy, not a 
          realistic model.
        </p>
        <p>
          The agent trades a single stock. Portfolio-level RL — where the agent allocates across 
          multiple assets — is substantially harder because the action space grows combinatorially. 
          This was scoped as a learning exercise, not a portfolio tool.
        </p>
        <p>
          DQN with a discrete action space (buy/hold/sell) cannot express fractional positions. A 
          continuous-action approach like DDPG or SAC would allow percentage-based allocation but 
          introduces training stability challenges.
        </p>
      </div>
    </section>

    <section class="project-section">
      <div class="container">
        <span class="project-section__label">Future Improvements</span>
        <h2 class="project-section__title">From single-stock toy to multi-asset research</h2>
        <ul class="project-list">
          <li>
            Replace DQN with PPO or SAC for continuous action spaces, allowing the agent to 
            express conviction as a position size rather than a binary buy/sell.
          </li>
          <li>
            Add realistic transaction cost modeling and test whether the agent's learned policy 
            survives after cost friction is applied.
          </li>
          <li>
            Extend to a multi-asset environment where the agent allocates across 5–10 correlated 
            equities, testing whether RL can learn diversification implicitly.
          </li>
          <li>
            Implement regime-conditional training: train separate agents on different market regimes, 
            with a meta-agent that selects which specialist to deploy.
          </li>
        </ul>
      </div>
    </section>

    <section class="section section--bordered" style="text-align: center;">
      <div class="container">
        <span class="section__label">Back to</span>
        <h2 class="section__title"><a href="../projects.html" style="border-bottom: 1px solid var(--color-border);">All Projects →</a></h2>
      </div>
    </section>

  </main>

  <footer class="footer">
    <div class="container container--wide">
      <div class="footer__inner">
        <span class="footer__text">Javian — Singapore</span>
        <ul class="footer__links">
          <li><a href="https://github.com" target="_blank" rel="noopener noreferrer">GitHub</a></li>
          <li><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer">LinkedIn</a></li>
          <li><a href="mailto:javian@email.com">Email</a></li>
        </ul>
      </div>
    </div>
  </footer>

  <script src="../script.js"></script>
</body>
</html>
